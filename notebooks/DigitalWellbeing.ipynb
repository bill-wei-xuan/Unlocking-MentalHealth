{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Merge Sensing, EMA, and student demographics Data\n",
    "\n",
    "sensing=pd.read_csv('../Data/sensing.csv')\n",
    "\n",
    "\n",
    "ema=pd.read_csv('../Data/general_ema.csv')\n",
    "\n",
    "demo=pd.read_csv('../Data/demographics.csv')\n",
    "\n",
    "\n",
    "data_merged=sensing.merge(ema,on=['uid','day'])   ### inner join\n",
    "\n",
    "data_full=data_merged.merge(demo,on='uid') ### inner join\n",
    "\n",
    "data_full=data_full[data_full['gender']!='both']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Raw Data Distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select relevant variables\n",
    "data_distribution=data_full[['uid','day','phq4_score','unlock_num_ep_0','unlock_duration_ep_0']]\n",
    "\n",
    "# Change unit of duration from seconds to minutes\n",
    "data_distribution['unlock_duration_ep_0']=data_distribution['unlock_duration_ep_0']/60\n",
    "\n",
    "# Generate duration per unlock\n",
    "data_distribution['duration_per_unlock_ep_0']=data_distribution['unlock_duration_ep_0']/data_distribution['unlock_num_ep_0']\n",
    "\n",
    "\n",
    "# Compute average, min, max, and standard deviation treating NaN as zero\n",
    "aggregations = {\n",
    "    'unlock_num_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'unlock_duration_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'duration_per_unlock_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'phq4_score': ['mean', 'min', 'max', 'std']\n",
    "}\n",
    "\n",
    "# Group by \"uid\" and calculate the statistics\n",
    "stats_with_zero = data_distribution.groupby('uid').agg(aggregations)\n",
    "\n",
    "# Flatten multi-level columns\n",
    "stats_with_zero.columns = ['_'.join(col) for col in stats_with_zero.columns]\n",
    "\n",
    "# Reset index to make \"uid\" a column again\n",
    "stats_with_zero = stats_with_zero.reset_index()\n",
    "\n",
    "# Compute min excluding zero\n",
    "def min_excluding_zero(series):\n",
    "    return series[series > 0].min() if any(series > 0) else np.nan\n",
    "\n",
    "# Group by \"uid\" and calculate min excluding zero\n",
    "min_excluding_zero_stats = data_distribution.groupby('uid').agg({\n",
    "    'unlock_num_ep_0': min_excluding_zero,\n",
    "    'unlock_duration_ep_0': min_excluding_zero,\n",
    "    'duration_per_unlock_ep_0': min_excluding_zero,\n",
    "    'phq4_score': min_excluding_zero\n",
    "}).rename(columns=lambda col: f\"{col}_min_excl_zero\")\n",
    "\n",
    "# Reset index to make \"uid\" a column again\n",
    "min_excluding_zero_stats = min_excluding_zero_stats.reset_index()\n",
    "\n",
    "# Merge both results\n",
    "final_df = pd.merge(stats_with_zero, min_excluding_zero_stats, on='uid', how='left')\n",
    "\n",
    "final_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General Distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Figure for Export\n",
    "#### With unusual values highlighted\n",
    "\n",
    "\n",
    "# List of groups and suffixes\n",
    "groups = [\"unlock_num_ep_0\", \"unlock_duration_ep_0\", \"duration_per_unlock_ep_0\"]\n",
    "suffixes = [\"_mean\", \"_max\"]\n",
    "\n",
    "# Initialize the figure\n",
    "fig, axes = plt.subplots(len(suffixes), len(groups), figsize=(20, 10), constrained_layout=True)\n",
    "\n",
    "\n",
    "#### num mean\n",
    "\n",
    "data = final_df['unlock_num_ep_0_mean'].dropna()\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(data.min() / 50) * 50  # Round down to the nearest 50\n",
    "max_value = np.ceil(data.max() / 50) * 50   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 50, 50)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "sns.histplot(data, kde=True, ax=axes[0, 0], bins=bin_edges, line_kws={'linewidth': 5})\n",
    "axes[0, 0].set_title(\"(a) Mean Unlock Number\", fontsize=30, fontweight='bold', pad=20)\n",
    "\n",
    "# Set custom bin edges as x-ticks\n",
    "axes[0, 0].set_xticks(bin_edges)\n",
    "axes[0, 0].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Convert edges to integers\n",
    "\n",
    "# Set y-axis label\n",
    "axes[0, 0].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 0].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Set x-axis label\n",
    "axes[0, 0].set_xlabel(\"Unit: Times\", fontsize=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration mean\n",
    "\n",
    "\n",
    "# Drop NaN values and convert data to hours\n",
    "data = final_df['unlock_duration_ep_0_mean'].dropna()\n",
    "data = data / 60  # Convert to hours\n",
    "\n",
    "# Define bins with a width of 1\n",
    "min_value = np.floor(data.min() / 1) * 1  # Round down to the nearest integer\n",
    "max_value = np.ceil(data.max() / 1) * 1   # Round up to the nearest integer\n",
    "bin_edges = np.arange(min_value, max_value + 1, 1)  # Create bins with width of 1\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "sns.histplot(data, kde=True, ax=axes[0, 1], bins=bin_edges, line_kws={'linewidth': 5})\n",
    "axes[0, 1].set_title(\"(b) Mean Unlock Duration\", fontsize=30,fontweight='bold', pad=20)  # Increased title font size\n",
    "\n",
    "# Set bin edges as x-ticks\n",
    "axes[0, 1].set_xticks(bin_edges)\n",
    "axes[0, 1].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Increased x-tick font size\n",
    "\n",
    "# Set y-axis label\n",
    "axes[0, 1].set_ylabel(\"\")  # No label for y-axis as per original code\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 1].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Set x-axis label\n",
    "axes[0, 1].set_xlabel('Unit: Hours', fontsize=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration/unlock mean\n",
    "data = final_df['duration_per_unlock_ep_0_mean'].dropna()\n",
    "\n",
    "# Define bins with a width of 1\n",
    "min_value = np.floor(data.min() / 2) * 2  # Round down to the nearest integer\n",
    "max_value = np.ceil(data.max() / 2) * 2   # Round up to the nearest integer\n",
    "bin_edges = np.arange(min_value, max_value + 2, 2)  # Create bins with width of 1\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "sns.histplot(data, kde=True, ax=axes[0, 2], bins=bin_edges,line_kws={'linewidth': 5})\n",
    "axes[0, 2].set_title(f\"(c) Mean Duration per Unlock\", fontsize=30, fontweight='bold', pad=20)  # Increased title font size\n",
    "\n",
    "# Set rounded bin edges as x-ticks\n",
    "axes[0, 2].set_xticks(bin_edges)\n",
    "axes[0, 2].set_xticklabels([f\"{int(edge)}\" if edge.is_integer() else f\"{edge:.1f}\" for edge in bin_edges], rotation=45, fontsize=30)\n",
    "#axes[0, 2].set_xticklabels(bin_edges_rounded, rotation=45, fontsize=20)  # Increased x-tick font size\n",
    "\n",
    "# Set y-axis label\n",
    "axes[0, 2].set_ylabel(\"\")\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 2].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "axes[0, 2].set_xlabel('Unit: Minutes', fontsize=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### num max\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "data = final_df['unlock_num_ep_0_max'].dropna()\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(data.min() / 100) * 100  # Round down to the nearest 50\n",
    "max_value = np.ceil(data.max() / 100) * 100   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 100, 100)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "hist = sns.histplot(data, kde=True, ax=axes[1, 0], bins=bin_edges, line_kws={'linewidth': 5})\n",
    "\n",
    "# Highlight bars whose bin edges contain zero\n",
    "for patch, left_edge in zip(hist.patches, bin_edges[:-1]):\n",
    "    if left_edge <= 0 < left_edge + 50:  # Check if the bin includes zero\n",
    "        patch.set_facecolor('red')  # Highlight in red\n",
    "\n",
    "# Format x-tick labels to display bin edges\n",
    "bin_edges_rounded = bin_edges.astype(int)\n",
    "axes[1, 0].set_xticks(bin_edges)\n",
    "axes[1, 0].set_xticklabels(bin_edges_rounded, rotation=45, fontsize=30)\n",
    "\n",
    "# Add title and labels\n",
    "axes[1, 0].set_title(\"(d) Max Unlock Number\", fontsize=30, fontweight='bold', pad=20)\n",
    "axes[1, 0].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "axes[1, 0].set_xlabel('Unit: Times', fontsize=30)\n",
    "\n",
    "# Adjust y-axis tick font size\n",
    "axes[1, 0].tick_params(axis='y', labelsize=25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration max\n",
    "\n",
    "# Drop NaN values from the data and convert to hours\n",
    "data = final_df['unlock_duration_ep_0_max'].dropna()\n",
    "data = data / 60  # Convert minutes to hours\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(data.min() / 2) * 2  # Round down to the nearest 50\n",
    "max_value = np.ceil(data.max() / 2) * 2  # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 2, 2)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "hist = sns.histplot(data, kde=True, ax=axes[1, 1], bins=bin_edges, line_kws={'linewidth': 5})\n",
    "\n",
    "# Highlight bars whose x-axis values are zero or above 1000\n",
    "for patch, left_edge in zip(hist.patches, bin_edges[:-1]):\n",
    "    if left_edge == 0:  # Highlight zero bins\n",
    "        patch.set_facecolor('red')\n",
    "    elif left_edge >= 16:  # Highlight bins above or equal to 1000\n",
    "        patch.set_facecolor('red')\n",
    "\n",
    "# Add title and labels\n",
    "axes[1, 1].set_title(f\"(e) Max Unlock Duration\", fontsize=30, fontweight='bold', pad=20)\n",
    "\n",
    "# Set x-ticks and labels\n",
    "axes[1, 1].set_xticks(bin_edges)\n",
    "axes[1, 1].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=25)\n",
    "\n",
    "# Set axis labels and font sizes\n",
    "axes[1, 1].set_ylabel(\"\")\n",
    "axes[1, 1].set_xlabel('Unit: Hours', fontsize=30)\n",
    "axes[1, 1].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration/unlock mean\n",
    "data = final_df['duration_per_unlock_ep_0_max'].dropna()\n",
    "\n",
    "data = data / 60  # Convert minutes to hours\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(data.min() / 1) * 1  # Round down to the nearest 50\n",
    "max_value = np.ceil(data.max() / 1) * 1  # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 1, 1)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "sns.histplot(data, kde=True, ax=axes[1, 2], bins=bin_edges,line_kws={'linewidth': 5})\n",
    "axes[1, 2].set_title(f\"(f) Max Duration per Unlock\", fontsize=30, fontweight='bold', pad=20)  # Increased title font size\n",
    "\n",
    "# Set rounded bin edges as x-ticks\n",
    "axes[1, 2].set_xticks(bin_edges)\n",
    "axes[1, 2].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Increased x-tick font size\n",
    "\n",
    "# Set y-axis label\n",
    "axes[1, 2].set_ylabel(\"\")\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[1, 2].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "axes[1, 2].set_xlabel('Unit: Hours', fontsize=30)\n",
    "\n",
    "\n",
    "# Iterate through each subplot and add bar heights\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        for bar in ax.patches:\n",
    "            height = bar.get_height()  # Get the height of the bar\n",
    "            if height > 0:  # Only annotate bars with height greater than 0\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,  # X-coordinate at the center of the bar\n",
    "                    height,  # Y-coordinate just above the bar\n",
    "                    f\"{int(height)}\",  # Convert height to integer for display\n",
    "                    ha='center', va='bottom', fontsize=27, color='black'  # Formatting\n",
    "                )\n",
    "\n",
    "# Export the figure as a PDF\n",
    "output_path = \"../Figures/distribution_highlight.pdf\"  # Change this to your desired file path\n",
    "fig.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution by Gender"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df_gender=final_df.merge(demo,on='uid')\n",
    "\n",
    "\n",
    "#### Figure for Export\n",
    "#### With unusual values highlighted\n",
    "\n",
    "# Define colors for genders\n",
    "colors = {'M': 'blue', 'F': 'red'}\n",
    "# List of groups and suffixes\n",
    "groups = [\"unlock_num_ep_0\", \"unlock_duration_ep_0\", \"duration_per_unlock_ep_0\"]\n",
    "suffixes = [\"_mean\", \"_max\"]\n",
    "\n",
    "# Initialize the figure\n",
    "fig, axes = plt.subplots(len(suffixes), len(groups), figsize=(20, 10), constrained_layout=True)\n",
    "\n",
    "\n",
    "#### num mean\n",
    "\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['unlock_num_ep_0_mean'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['unlock_num_ep_0_mean'].dropna()\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 50) * 50  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 50) * 50   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 50, 50)  # Create bins with width of 50\n",
    "\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[0, 0], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[0, 0], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "# Add title and labels\n",
    "axes[0, 0].set_title(\"(a) Mean Unlock Number\", fontsize=30, fontweight='bold')\n",
    "axes[0, 0].set_xlabel(\"Unit: Times\", fontsize=30)\n",
    "axes[0, 0].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "\n",
    "# Set custom bin edges as x-ticks\n",
    "axes[0, 0].set_xticks(bin_edges)\n",
    "axes[0, 0].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Convert edges to integers\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 0].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[0, 0].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration mean\n",
    "\n",
    "\n",
    "# Drop NaN values and convert data to hours\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['unlock_duration_ep_0_mean'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['unlock_duration_ep_0_mean'].dropna()\n",
    "\n",
    "data_male=data_male/60\n",
    "data_female=data_female/60\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 1) * 1  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 1) * 1   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 1, 1)  # Create bins with width of 50\n",
    "\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[0, 1], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[0, 1], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "# Add title and labels\n",
    "axes[0, 1].set_title(\"(b) Mean Unlock Duration\", fontsize=30, fontweight='bold')\n",
    "axes[0, 1].set_xlabel(\"Unit: Hours\", fontsize=30)\n",
    "axes[0, 1].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "\n",
    "# Set custom bin edges as x-ticks\n",
    "axes[0, 1].set_xticks(bin_edges)\n",
    "axes[0, 1].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Convert edges to integers\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 1].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[0, 1].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration/unlock mean\n",
    "\n",
    "# Drop NaN values and convert data to hours\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['duration_per_unlock_ep_0_mean'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['duration_per_unlock_ep_0_mean'].dropna()\n",
    "\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 2) * 2  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 2) * 2   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 2, 2)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[0, 2], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[0, 2], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "axes[0, 2].set_title(f\"(c) Mean Duration per Unlock\", fontsize=30, fontweight='bold')  # Increased title font size\n",
    "\n",
    "# Set rounded bin edges as x-ticks\n",
    "axes[0, 2].set_xticks(bin_edges)\n",
    "axes[0, 2].set_xticklabels([f\"{int(edge)}\" if edge.is_integer() else f\"{edge:.1f}\" for edge in bin_edges], rotation=45, fontsize=30)\n",
    "#axes[0, 2].set_xticklabels(bin_edges_rounded, rotation=45, fontsize=20)  # Increased x-tick font size\n",
    "\n",
    "# Set y-axis label\n",
    "axes[0, 2].set_ylabel(\"\")\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[0, 2].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "axes[0, 2].set_xlabel('Unit: Minutes', fontsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[0, 2].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "#### num max\n",
    "\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['unlock_num_ep_0_max'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['unlock_num_ep_0_max'].dropna()\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 100) * 100  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 100) * 100   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 100, 100)  # Create bins with width of 50\n",
    "\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[1, 0], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[1, 0], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "# Add title and labels\n",
    "axes[1, 0].set_title(\"(d) Max Unlock Number\", fontsize=30, fontweight='bold')\n",
    "axes[1, 0].set_xlabel(\"Unit: Times\", fontsize=30)\n",
    "axes[1, 0].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "\n",
    "# Set custom bin edges as x-ticks\n",
    "axes[1, 0].set_xticks(bin_edges)\n",
    "axes[1, 0].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Convert edges to integers\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[1, 0].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[1, 0].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration max\n",
    "\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['unlock_duration_ep_0_max'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['unlock_duration_ep_0_max'].dropna()\n",
    "\n",
    "data_male=data_male/60\n",
    "data_female=data_female/60\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 2) * 2  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 2) * 2   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 2, 2)  # Create bins with width of 50\n",
    "\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[1, 1], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[1, 1], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "# Add title and labels\n",
    "axes[1, 1].set_title(\"(e) Max Unlock Duration\", fontsize=30, fontweight='bold')\n",
    "axes[1, 1].set_xlabel(\"Unit: Hours\", fontsize=30)\n",
    "axes[1, 1].set_ylabel(\"Number of Students\", fontsize=30)\n",
    "\n",
    "# Set custom bin edges as x-ticks\n",
    "axes[1, 1].set_xticks(bin_edges)\n",
    "axes[1, 1].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=25)  # Convert edges to integers\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[1, 1].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[1, 1].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### duration/unlock mean\n",
    "# Filter data by gender\n",
    "data_male = final_df_gender[final_df_gender['gender'] == 'M']['duration_per_unlock_ep_0_max'].dropna()\n",
    "data_female = final_df_gender[final_df_gender['gender'] == 'F']['duration_per_unlock_ep_0_max'].dropna()\n",
    "\n",
    "data_male=data_male/60\n",
    "data_female=data_female/60\n",
    "\n",
    "# Define bins with a width of 50\n",
    "min_value = np.floor(min(data_male.min(), data_female.min()) / 1) * 1  # Round down to the nearest 50\n",
    "max_value = np.ceil(max(data_male.max(), data_female.max()) / 1) * 1   # Round up to the nearest 50\n",
    "bin_edges = np.arange(min_value, max_value + 1, 1)  # Create bins with width of 50\n",
    "\n",
    "# Plot histogram using seaborn with custom bins\n",
    "# Plot histograms for male and female data\n",
    "sns.histplot(data_male, kde=True, bins=bin_edges, ax=axes[1, 2], color='blue', label='Male', line_kws={'linewidth': 4}, alpha=0.4)\n",
    "sns.histplot(data_female, kde=True, bins=bin_edges, ax=axes[1, 2], color='red', label='Female', line_kws={'linewidth': 4},alpha=0.4)\n",
    "\n",
    "axes[1, 2].set_title(f\"(f) Max Duration per Unlock\", fontsize=30, fontweight='bold')  # Increased title font size\n",
    "\n",
    "# Set rounded bin edges as x-ticks\n",
    "axes[1, 2].set_xticks(bin_edges)\n",
    "axes[1, 2].set_xticklabels(bin_edges.astype(int), rotation=45, fontsize=30)  # Increased x-tick font size\n",
    "\n",
    "# Set y-axis label\n",
    "axes[1, 2].set_ylabel(\"\")\n",
    "\n",
    "# Increase y-tick font size\n",
    "axes[1, 2].tick_params(axis='y', labelsize=30)\n",
    "\n",
    "axes[1, 2].set_xlabel('Unit: Hours', fontsize=30)\n",
    "\n",
    "# Add legend\n",
    "axes[1, 2].legend(title='Gender', fontsize=20, title_fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Export the figure as a PDF\n",
    "output_path = \"../Figures/distribution_by_gender.pdf\"  # Change this to your desired file path\n",
    "fig.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Pearson Correlation Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data after exclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Convert hours to minutes\n",
    "data_full['unlock_duration_ep_0']=data_full['unlock_duration_ep_0']/60\n",
    "\n",
    "## exclude rows whose duration is over 16 hours\n",
    "data_new=data_full[data_full['unlock_duration_ep_0']<=1000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_distribution_new=data_new[['uid','day','phq4_score','unlock_num_ep_0','unlock_duration_ep_0']]\n",
    "\n",
    "#data_distribution['unlock_duration_ep_0']=data_distribution['unlock_duration_ep_0']/60\n",
    "\n",
    "data_distribution_new['duration_per_unlock_ep_0']=data_distribution_new['unlock_duration_ep_0']/data_distribution_new['unlock_num_ep_0']\n",
    "\n",
    "\n",
    "# Assume df is your dataframe\n",
    "# Replace NaN with zero for calculation\n",
    "#df_filled = data_distribution.fillna(0)\n",
    "#df_filled['duration_per_unlock_ep_0']=df_filled['unlock_duration_ep_0']/df_filled['unlock_num_ep_0']\n",
    "\n",
    "# Compute average, min, max, and standard deviation treating NaN as zero\n",
    "aggregations = {\n",
    "    'unlock_num_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'unlock_duration_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'duration_per_unlock_ep_0': ['mean', 'min', 'max', 'std'],\n",
    "    'phq4_score': ['mean', 'min', 'max', 'std']\n",
    "}\n",
    "\n",
    "# Group by \"uid\" and calculate the statistics\n",
    "stats_with_zero = data_distribution_new.groupby('uid').agg(aggregations)\n",
    "\n",
    "# Flatten multi-level columns\n",
    "stats_with_zero.columns = ['_'.join(col) for col in stats_with_zero.columns]\n",
    "\n",
    "# Reset index to make \"uid\" a column again\n",
    "stats_with_zero = stats_with_zero.reset_index()\n",
    "\n",
    "# Compute min excluding zero\n",
    "def min_excluding_zero(series):\n",
    "    return series[series > 0].min() if any(series > 0) else np.nan\n",
    "\n",
    "# Group by \"uid\" and calculate min excluding zero\n",
    "min_excluding_zero_stats = data_distribution_new.groupby('uid').agg({\n",
    "    'unlock_num_ep_0': min_excluding_zero,\n",
    "    'unlock_duration_ep_0': min_excluding_zero,\n",
    "    'duration_per_unlock_ep_0': min_excluding_zero,\n",
    "    'phq4_score': min_excluding_zero\n",
    "}).rename(columns=lambda col: f\"{col}_min_excl_zero\")\n",
    "\n",
    "# Reset index to make \"uid\" a column again\n",
    "min_excluding_zero_stats = min_excluding_zero_stats.reset_index()\n",
    "\n",
    "# Merge both results\n",
    "final_df_new = pd.merge(stats_with_zero, min_excluding_zero_stats, on='uid', how='left')\n",
    "\n",
    "final_df_new\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##\n",
    "data_new=data_new.merge(final_df_new,on='uid',how='left')\n",
    "\n",
    "data_new=data_new[(data_new['unlock_num_ep_0_std']>0) & (data_new['unlock_duration_ep_0_std']>0) & (data_new['phq4_score_std']>0)]\n",
    "\n",
    "data_new=data_new[(data_new['unlock_num_ep_0_max']>0) & (data_new['unlock_duration_ep_0_max']>0)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=data_new\n",
    "\n",
    "df['day'] = pd.to_datetime(df['day'], format='%Y%m%d')\n",
    "\n",
    "df=df.fillna(0).copy()\n",
    "\n",
    "## Generate Moving Average\n",
    "\n",
    "merged_df1= df.sort_values(by=['uid', 'day'])\n",
    "\n",
    "window_size = 14\n",
    "newdf=merged_df1\n",
    "newdf\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to calculate rolling metrics for all columns except 'UID' and 'day'\n",
    "def calculate_rolling_metrics(group):\n",
    "    col_to_avg=[\n",
    "     'unlock_duration_ep_0',\n",
    "     'unlock_num_ep_0',\n",
    "\n",
    "     'unlock_duration_ep_1',\n",
    "     'unlock_num_ep_1',\n",
    "\n",
    "     'unlock_duration_ep_2',\n",
    "     'unlock_num_ep_2',\n",
    "\n",
    "     'unlock_duration_ep_3',\n",
    "     'unlock_num_ep_3',\n",
    "\n",
    "     'loc_self_dorm_unlock_duration',\n",
    "     'loc_self_dorm_unlock_num',\n",
    "\n",
    "     'loc_study_unlock_duration',\n",
    "     'loc_study_unlock_num',\n",
    "\n",
    "     'loc_social_unlock_duration',\n",
    "     'loc_social_unlock_num',\n",
    "\n",
    "     'loc_food_unlock_duration',\n",
    "     'loc_food_unlock_num',\n",
    "\n",
    "     'loc_home_unlock_duration',\n",
    "     'loc_home_unlock_num',\n",
    "     ]\n",
    "\n",
    "    group = group.sort_values('day')  # Ensure sorting within each group\n",
    "\n",
    "    # Apply rolling calculations for each numeric column\n",
    "    for col in group.columns:\n",
    "        if col not in ['uid', 'day'] and col in col_to_avg:  # Skip non-numeric columns\n",
    "            group[f'{col}_2w_sum'] = group.rolling('14D', on='day')[col].sum()\n",
    "            group[f'{col}_2w_med'] = group.rolling('14D', on='day')[col].median()\n",
    "            group[f'{col}_2w_avg'] = group.rolling('14D', on='day')[col].mean()\n",
    "\n",
    "    return group\n",
    "\n",
    "# # Group by UID and apply the rolling function\n",
    "# newdf_mod = (\n",
    "#     newdf_mod\n",
    "#     .groupby('uid', group_keys=False)\n",
    "#     .apply(calculate_rolling_metrics)\n",
    "# )\n",
    "\n",
    "# newdf_mod\n",
    "\n",
    "newdf = (\n",
    "    newdf\n",
    "    .groupby('uid', group_keys=False)\n",
    "    .apply(calculate_rolling_metrics)\n",
    ")\n",
    "\n",
    "newdf\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newdf_everything=newdf[['uid',\n",
    " 'day', 'gender',\n",
    " 'unlock_duration_ep_0',\n",
    " 'unlock_num_ep_0',\n",
    " 'unlock_duration_ep_1',\n",
    " 'unlock_num_ep_1',\n",
    " 'unlock_duration_ep_2',\n",
    " 'unlock_num_ep_2',\n",
    " 'unlock_duration_ep_3',\n",
    " 'unlock_num_ep_3',\n",
    " 'loc_self_dorm_unlock_duration',\n",
    " 'loc_self_dorm_unlock_num',\n",
    " 'loc_study_unlock_duration',\n",
    " 'loc_study_unlock_num',\n",
    " 'loc_social_unlock_duration',\n",
    " 'loc_social_unlock_num',\n",
    " 'loc_food_unlock_duration',\n",
    " 'loc_food_unlock_num',\n",
    " 'loc_home_unlock_duration',\n",
    " 'loc_home_unlock_num',\n",
    " 'unlock_duration_ep_0_2w_avg', 'unlock_duration_ep_0_2w_med',\n",
    "       'unlock_duration_ep_0_2w_sum', 'unlock_num_ep_0_2w_avg',\n",
    "       'unlock_num_ep_0_2w_med', 'unlock_num_ep_0_2w_sum',\n",
    "       'unlock_duration_ep_1_2w_avg', 'unlock_duration_ep_1_2w_med',\n",
    "       'unlock_duration_ep_1_2w_sum', 'unlock_num_ep_1_2w_avg',\n",
    "       'unlock_num_ep_1_2w_med', 'unlock_num_ep_1_2w_sum',\n",
    "       'unlock_duration_ep_2_2w_avg', 'unlock_duration_ep_2_2w_med',\n",
    "       'unlock_duration_ep_2_2w_sum', 'unlock_num_ep_2_2w_avg',\n",
    "       'unlock_num_ep_2_2w_med', 'unlock_num_ep_2_2w_sum',\n",
    "       'unlock_duration_ep_3_2w_avg', 'unlock_duration_ep_3_2w_med',\n",
    "       'unlock_duration_ep_3_2w_sum', 'unlock_num_ep_3_2w_avg',\n",
    "       'unlock_num_ep_3_2w_med', 'unlock_num_ep_3_2w_sum',\n",
    "       'loc_self_dorm_unlock_duration_2w_avg',\n",
    "       'loc_self_dorm_unlock_duration_2w_med',\n",
    "       'loc_self_dorm_unlock_duration_2w_sum',\n",
    "       'loc_self_dorm_unlock_num_2w_avg', 'loc_self_dorm_unlock_num_2w_med',\n",
    "       'loc_self_dorm_unlock_num_2w_sum', 'loc_study_unlock_duration_2w_avg',\n",
    "       'loc_study_unlock_duration_2w_med',\n",
    "       'loc_study_unlock_duration_2w_sum', 'loc_study_unlock_num_2w_avg',\n",
    "       'loc_study_unlock_num_2w_med', 'loc_study_unlock_num_2w_sum',\n",
    "       'loc_social_unlock_duration_2w_avg',\n",
    "       'loc_social_unlock_duration_2w_med',\n",
    "       'loc_social_unlock_duration_2w_sum', 'loc_social_unlock_num_2w_avg',\n",
    "       'loc_social_unlock_num_2w_med', 'loc_social_unlock_num_2w_sum',\n",
    "       'loc_food_unlock_duration_2w_avg', 'loc_food_unlock_duration_2w_med',\n",
    "       'loc_food_unlock_duration_2w_sum', 'loc_food_unlock_num_2w_avg',\n",
    "       'loc_food_unlock_num_2w_med', 'loc_food_unlock_num_2w_sum',\n",
    "       'loc_home_unlock_duration_2w_avg', 'loc_home_unlock_duration_2w_med',\n",
    "       'loc_home_unlock_duration_2w_sum', 'loc_home_unlock_num_2w_avg',\n",
    "       'loc_home_unlock_num_2w_med', 'loc_home_unlock_num_2w_sum','phq4_score']]\n",
    "\n",
    "newdf_everything.head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newdf_everything=newdf_everything.dropna(subset=[\n",
    "       'phq4_score']).copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newdf_everything.to_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pearson Correlation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_corr=pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n",
    "\n",
    "demo=pd.read_csv('../Data/Demographics/demographics.csv')\n",
    "\n",
    "data_corr=data_corr.merge(demo,on='uid')\n",
    "\n",
    "## Generate Duration per Unlock\n",
    "data_corr['duration_per_unlock_ep_0_2w_avg']=data_corr['unlock_duration_ep_0_2w_avg']/data_corr['unlock_num_ep_0_2w_avg']\n",
    "\n",
    "## Change Units\n",
    "data_corr['duration_per_unlock_ep_0_2w_avg']=data_corr['duration_per_unlock_ep_0_2w_avg']/60\n",
    "\n",
    "data_corr['unlock_duration_ep_0_2w_avg']=data_corr['unlock_duration_ep_0_2w_avg']/3600\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overall and Gender Specific"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate correlation and p-value with cleaning\n",
    "def calculate_corr_and_pval(data, var):\n",
    "    clean_data = data[['phq4_score', var]].dropna()  # Drop rows with NaNs\n",
    "    clean_data = clean_data[np.isfinite(clean_data['phq4_score']) & np.isfinite(clean_data[var])]  # Drop rows with infs\n",
    "    if len(clean_data) > 1:  # Ensure there are enough data points\n",
    "        corr, pval = pearsonr(clean_data['phq4_score'], clean_data[var])\n",
    "    else:\n",
    "        corr, pval = np.nan, np.nan  # Return NaN if not enough data\n",
    "    return corr, pval\n",
    "\n",
    "# Filter the data for male and female\n",
    "data_male = data_corr[data_corr['gender'] == 'M']\n",
    "data_female = data_corr[data_corr['gender'] == 'F']\n",
    "\n",
    "# Define the variables for correlation\n",
    "variables = ['unlock_duration_ep_0_2w_avg', 'unlock_num_ep_0_2w_avg', 'duration_per_unlock_ep_0_2w_avg']\n",
    "\n",
    "# Initialize dictionaries for correlations and p-values\n",
    "results = {'Overall Correlation': [], 'Overall P-value': [],\n",
    "           'Male Correlation': [], 'Male P-value': [],\n",
    "           'Female Correlation': [], 'Female P-value': []}\n",
    "\n",
    "# Loop through variables and calculate correlations and p-values\n",
    "for var in variables:\n",
    "    # Overall\n",
    "    corr, pval = calculate_corr_and_pval(data_corr, var)\n",
    "    results['Overall Correlation'].append(corr)\n",
    "    results['Overall P-value'].append(pval)\n",
    "\n",
    "    # Male\n",
    "    corr, pval = calculate_corr_and_pval(data_male, var)\n",
    "    results['Male Correlation'].append(corr)\n",
    "    results['Male P-value'].append(pval)\n",
    "\n",
    "    # Female\n",
    "    corr, pval = calculate_corr_and_pval(data_female, var)\n",
    "    results['Female Correlation'].append(corr)\n",
    "    results['Female P-value'].append(pval)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "correlation_table = pd.DataFrame(results, index=['Unlock Duration', 'Unlock Number', 'Duration per Unlock'])\n",
    "\n",
    "# Export the table to a CSV file\n",
    "output_file = '../Tables/correlation_table_with_pvalues.csv'\n",
    "correlation_table.to_csv(output_file)\n",
    "\n",
    "# Display the table\n",
    "print(correlation_table)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Location"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "locations = ['study', 'social', 'food', 'home', 'self_dorm']\n",
    "\n",
    "for loc in locations:\n",
    "    # Create duration per unlock feature\n",
    "    data_corr[f'loc_{loc}_duration_per_unlock_2w_avg'] = data_corr[f'loc_{loc}_unlock_duration_2w_avg'] / data_corr[f'loc_{loc}_unlock_num_2w_avg']\n",
    "\n",
    "    # Filter the data for male and female\n",
    "    data_male = data_corr[data_corr['gender'] == 'M']\n",
    "    data_female = data_corr[data_corr['gender'] == 'F']\n",
    "\n",
    "    # Define the variables for correlation\n",
    "    variables = [f'loc_{loc}_duration_per_unlock_2w_avg', f'loc_{loc}_unlock_duration_2w_avg', f'loc_{loc}_unlock_num_2w_avg']\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {'Overall Correlation': [], 'Overall P-value': [],\n",
    "               'Male Correlation': [], 'Male P-value': [],\n",
    "               'Female Correlation': [], 'Female P-value': []}\n",
    "\n",
    "    # Loop through variables and calculate correlations and p-values\n",
    "    for var in variables:\n",
    "        # Overall\n",
    "        overall_corr, overall_pval = pearsonr(data_corr.dropna(subset=['phq4_score', var])['phq4_score'],\n",
    "                                              data_corr.dropna(subset=['phq4_score', var])[var])\n",
    "        results['Overall Correlation'].append(overall_corr)\n",
    "        results['Overall P-value'].append(overall_pval)\n",
    "\n",
    "        # Male\n",
    "        male_corr, male_pval = pearsonr(data_male.dropna(subset=['phq4_score', var])['phq4_score'],\n",
    "                                        data_male.dropna(subset=['phq4_score', var])[var])\n",
    "        results['Male Correlation'].append(male_corr)\n",
    "        results['Male P-value'].append(male_pval)\n",
    "\n",
    "        # Female\n",
    "        female_corr, female_pval = pearsonr(data_female.dropna(subset=['phq4_score', var])['phq4_score'],\n",
    "                                            data_female.dropna(subset=['phq4_score', var])[var])\n",
    "        results['Female Correlation'].append(female_corr)\n",
    "        results['Female P-value'].append(female_pval)\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    correlation_table = pd.DataFrame(results, index=['Duration per Unlock', 'Unlock Duration', 'Unlock Number'])\n",
    "\n",
    "    # Export the table to a CSV file\n",
    "    output_file = f'../Tables/correlation_table_{loc}.csv'\n",
    "    correlation_table.to_csv(output_file)\n",
    "\n",
    "    # Display the table\n",
    "    print(f\"Correlation Table for {loc.capitalize()}:\")\n",
    "    print(correlation_table)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Logistic Regression Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n",
    "\n",
    "\n",
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'ep_0' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Normal': log_reg.coef_[0],\n",
    "    'Coefficient_Mild': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gender Specific - Male"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n",
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)\n",
    "\n",
    "filtered_df = df[df['gender'] == 'M']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df.columns\n",
    "features = []\n",
    "\n",
    "for c in list(filtered_df.columns):\n",
    "    if 'ep_0' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "\n",
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df[target] = label_encoder.fit_transform(filtered_df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = filtered_df[features]\n",
    "y = filtered_df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gender - Female"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n",
    "\n",
    "\n",
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = df[df['gender'] == 'F']\n",
    "filtered_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(filtered_df.columns):\n",
    "    if 'ep_0' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df[target] = label_encoder.fit_transform(filtered_df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = filtered_df[features]\n",
    "y = filtered_df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location -food"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df= pd.read_csv('/Users/meghnaroy/Library/CloudStorage/OneDrive-purdue.edu/Research/MentalHealth/MOBISOFT/Formatted_Not_Normalized_Everything.csv')\n",
    "# df\n",
    "\n",
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'food' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### location - Social"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'social' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location - home"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'home' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location -DORM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'dorm' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Location - study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df= pd.read_csv('../Data/Formatted_Not_Normalized_Everything_Mobisoft.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, categorize the PHQ4 scores into the specified categories\n",
    "def categorize_phq4(score):\n",
    "    if 0 <= score <= 2:\n",
    "        return 'Normal'\n",
    "    elif 3 <= score <= 5:\n",
    "        return 'Mild'\n",
    "    elif 6 <= score <= 8:\n",
    "        return 'Moderate'\n",
    "    elif 9 <= score <= 12:\n",
    "        return 'Severe'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add a new column for PHQ4 category\n",
    "df['phq4_category'] = df['phq4_score'].apply(categorize_phq4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dense, Flatten, Dropout\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features=[]\n",
    "\n",
    "for c in list(df.columns):\n",
    "    if 'study' in c:\n",
    "        if 'avg' in c:\n",
    "            features.append(c)\n",
    "\n",
    "features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = 'phq4_category'\n",
    "\n",
    "# Encode the target variable (phq4_category) into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[target] = label_encoder.fit_transform(df[target])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform multi-class logistic regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000 , C=0.1)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Find the coefficients of each feature\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mild': log_reg.coef_[0],\n",
    "    'Coefficient_Normal': log_reg.coef_[1],\n",
    "    'Coefficient_Moderate': log_reg.coef_[2],\n",
    "    'Coefficient_Severe': log_reg.coef_[3],\n",
    "})\n",
    "coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add an intercept to the feature matrix\n",
    "X_train_with_intercept = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Fit the multinomial logistic regression model using statsmodels\n",
    "mnlogit_model = sm.MNLogit(y_train, X_train_with_intercept)\n",
    "\n",
    "# Custom fit function with L2 regularization\n",
    "def fit_with_regularization(model, reg_param=0.1, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Fits the MNLogit model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model: statsmodels.MNLogit instance\n",
    "        reg_param: Regularization parameter (similar to 1/C in sklearn)\n",
    "        maxiter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Result object similar to statsmodels fit\n",
    "    \"\"\"\n",
    "    # Initialize starting parameters as zeros\n",
    "    start_params = np.zeros(model.exog.shape[1] * (model.J - 1))\n",
    "\n",
    "    def regularized_loglike(params):\n",
    "        loglike = model.loglike(params)  # Log-likelihood from MNLogit\n",
    "        penalty = reg_param * np.sum(params**2)  # L2 penalty\n",
    "        return -(loglike - penalty)  # Negative log-likelihood for minimization\n",
    "\n",
    "    def regularized_score(params):\n",
    "        score = model.score(params)  # Score (gradient) from MNLogit\n",
    "        penalty_gradient = 2 * reg_param * params  # Gradient of L2 penalty\n",
    "        return -(score - penalty_gradient)\n",
    "\n",
    "    # Use 'lbfgs' solver for optimization\n",
    "    result = minimize(\n",
    "        fun=regularized_loglike,\n",
    "        x0=start_params,\n",
    "        jac=regularized_score,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': maxiter, 'disp': True},\n",
    "    )\n",
    "\n",
    "    # Validate convergence and return parameters\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization did not converge: \" + result.message)\n",
    "\n",
    "    # Wrap the results for compatibility with statsmodels API\n",
    "    return model.fit(start_params=result.x, maxiter=maxiter, method='lbfgs', disp=False)\n",
    "\n",
    "# Fit the model with L2 regularization\n",
    "result = fit_with_regularization(mnlogit_model, reg_param=0.1, maxiter=1000)\n",
    "\n",
    "# Display the summary with p-values\n",
    "print(result.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
